# readme

标签（空格分隔）： work

---
##requirements:
the model folder and the scripts folder should be Peer catalogue
the model folder should have the same name with the model data eg:lenet folder,the model data in it should be lenet.caffemodel lenet.prototxt 
channel_mean_value.txt, dataset.txt, reorder_channel.txt and one image should be included in the folder
##usage of the scripts：
###for import.sh
./import.sh modelname
result: it will generate 2 file,one data file store the NN(nerual network) paraments,one json file store the NN structure.
fp: different modeltype(tf\caffe\tflite...)

###for quantize.sh
./quantize.sh modelname int8
paraments need to caution: (int8 means the datatype you want to quantize)
inner quantization paraments:
--batch-size 1  :the model batchsize,(not sure if useful for quantization)
--dtype float  :datatype used for caculation
--quantized-dtype  :target dtype to quantize
--model-quantize   :output file(eg : lenet_int8.quantize)
--model-input  :jsonfile(lenet.json)
--model-data  :datafile(lenet.data)
--reorder--channel  :caffe(2 1 0) tf(0 1 2)
--channel-mean-value  :127 127 127 256 means (B-127)/256 (G-127)/256 (R-127)/256
--source :text or sqlite
--source-file  :.txt or .dsx  txt file store the input image and (label)
result: generate a .quantize file


###for export_ovx_data.sh
./export_ovx_data.sh lenet int8
inner ovxgenerator paraments:
--data-input  :.data file
--model-input  :.json file
--model--quantize  :.quantize file generated by quantize.sh
--model-output  :model output directory and output filename
--export-dtype  :float or quantized
--channel-mean-value  :the same with above
--reorder-data  :the same with above
--target-ide-project  :default(linux64) win32
--batch-size  :batched for exported application code
result: generate the ovx project



